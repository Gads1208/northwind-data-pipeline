## ‚úÖ STATUS DO PROJETO

**Containers Docker**: ‚úÖ Rodando (4 containers)
- ‚úÖ PostgreSQL com dados Northwind (5 clientes, 5 pedidos)
- ‚úÖ Airflow Webserver (http://localhost:8080)
- ‚úÖ Airflow Scheduler
- ‚úÖ Airflow Database

**Nota sobre Ingest√£o**: Este projeto usa **scripts Python customizados no Airflow** para extrair dados do PostgreSQL e carregar no BigQuery, eliminando a necessidade do Airbyte. Veja [airflow/scripts/postgres_to_bigquery.py](airflow/scripts/postgres_to_bigquery.py) para detalhes.

---

## üöÄ Quick Start (Vers√£o Simplificada)

### 1. Verifique os containers

```bash
docker-compose ps
```

### 2. Acesse os servi√ßos

- **Airflow UI**: http://localhost:8080
  - Username: `airflow`
  - Password: `airflow`

- **PostgreSQL**: `localhost:5432`
  - Database: `northwind`
  - Username: `postgres`
  - Password: `postgres`

### 3. Verifique os dados no Postgres

```bash
docker exec -it northwind-postgres psql -U postgres -d northwind
```

```sql
\dt                              -- Listar tabelas
SELECT COUNT(*) FROM customers;  -- Ver dados
SELECT * FROM orders LIMIT 5;    -- Ver pedidos
\q                               -- Sair
```

### 4. Configurar Google Cloud Platform

Antes de executar o pipeline completo, voc√™ precisa:

1. **Criar projeto no GCP** e habilitar BigQuery API
2. **Criar Service Account** e baixar a chave JSON
3. **Salvar a chave** como `gcp-key.json` na raiz do projeto
4. **Configurar vari√°vel de ambiente**:

```bash
# Editar arquivo .env
echo "GCP_PROJECT_ID=seu-projeto-id" > .env
```

5. **Criar datasets no BigQuery**:

```sql
CREATE SCHEMA northwind_bronze;
CREATE SCHEMA northwind_silver;
CREATE SCHEMA northwind_gold;
```

### 5. Reiniciar containers com as novas configura√ß√µes

```bash
docker-compose down
docker-compose up -d
```

### 6. Executar o pipeline completo no Airflow

1. Acesse http://localhost:8080
2. Encontre o DAG `northwind_data_pipeline`
3. Clique em "Trigger DAG"

O DAG ir√°:
- ‚úÖ Extrair dados do PostgreSQL
- ‚úÖ Carregar no BigQuery (camada Bronze)
- ‚úÖ Executar transforma√ß√µes dbt (Silver ‚Üí Gold)
- ‚úÖ Gerar documenta√ß√£o

### 7. Ou execute manualmente passo a passo

```bash
# Entrar no container do Airflow
docker exec -it airflow-webserver bash

# Testar o script de ingest√£o
cd /opt/airflow/scripts
python postgres_to_bigquery.py

# Navegar para o projeto dbt
cd /opt/airflow/dbt/northwind_dw

# Executar transforma√ß√µes
dbt run --profiles-dir /opt/airflow/dbt

# Executar testes
dbt test --profiles-dir /opt/airflow/dbt
```

---

## üìù Configura√ß√£o Completa

Para configura√ß√£o detalhada, veja:
- [docs/SETUP.md](docs/SETUP.md) - Guia completo de instala√ß√£o
- [docs/AIRBYTE_SETUP.md](docs/AIRBYTE_SETUP.md) - Op√ß√µes para ingest√£o de dados
- [docs/CHECKLIST.md](docs/CHECKLIST.md) - Checklist de implementa√ß√£o

---

## üõ†Ô∏è Comandos √öteis

```bash
# Ver logs
docker-compose logs -f

# Reiniciar servi√ßos
docker-compose restart

# Parar tudo
docker-compose down

# Parar e remover volumes (limpa dados)
docker-compose down -v

# Ver uso de recursos
docker stats
```

---

## üìä Estrutura do Pipeline

```
PostgreSQL (Northwind DB)
    ‚Üì
Python Script (Airflow)
    ‚Üì
BigQuery Bronze Layer
    ‚Üì
dbt Transformations (Silver ‚Üí Gold)
    ‚Üì
Analytics Ready!
```

**Destaques da Implementa√ß√£o:**
- ‚úÖ **Ingest√£o customizada** com Python (sem depend√™ncias externas)
- ‚úÖ **Schema mapping autom√°tico** PostgreSQL ‚Üí BigQuery
- ‚úÖ **Metadados de rastreamento** (`_airbyte_extracted_at`, `_airbyte_loaded_at`)
- ‚úÖ **Tratamento de erros** e logging detalhado
- ‚úÖ **Execu√ß√£o paralela** de tabelas (quando aplic√°vel)

---

## üéØ Para Demonstra√ß√£o em Portf√≥lio

Este projeto demonstra:

‚úÖ **Engenharia de Dados** - Pipeline completo end-to-end
‚úÖ **Python** - Scripts customizados para ETL (500+ linhas)
‚úÖ **Arquitetura de Dados** - Medallion (Bronze/Silver/Gold)
‚úÖ **Modelagem** - Star Schema com dbt
‚úÖ **Orquestra√ß√£o** - Apache Airflow com DAGs complexos
‚úÖ **Cloud** - Google BigQuery
‚úÖ **DevOps** - Docker, Docker Compose, IaC
‚úÖ **Documenta√ß√£o** - Completa e profissional

**Diferencial**: Ao usar scripts Python customizados em vez de ferramentas prontas, voc√™ demonstra:
- Dom√≠nio de Python e bibliotecas (psycopg2, google-cloud-bigquery)
- Capacidade de criar solu√ß√µes sob medida
- Conhecimento profundo de ETL e integra√ß√£o de sistemas
- Habilidade de trabalhar sem depender apenas de ferramentas comerciais

---

**Pr√≥ximos passos**: Veja [PROJECT_SUMMARY.md](PROJECT_SUMMARY.md) para continuar!
