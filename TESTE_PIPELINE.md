## üîß Teste do Pipeline - Passo a Passo

### ‚úÖ Status Atual

**Containers Docker**: Todos rodando!
- ‚úÖ PostgreSQL: 51 registros em 8 tabelas
- ‚úÖ Airflow: Webserver e Scheduler ativos
- ‚úÖ Script Python: Funcionando (bibliotecas instaladas)

**Configura√ß√£o GCP**:
- ‚úÖ Arquivo gcp-key.json presente (2.4KB)
- ‚úÖ Projeto: portifolio-482811
- ‚ùå **Datasets precisam ser criados manualmente**

---

## üöÄ Pr√≥ximo Passo: Criar Datasets no BigQuery

A Service Account atual n√£o tem permiss√£o para criar datasets. Voc√™ precisa criar manualmente:

### **Op√ß√£o 1: Via Console do GCP** (Recomendado)

1. Acesse https://console.cloud.google.com/bigquery?project=portifolio-482811

2. No painel esquerdo, clique no seu projeto `portifolio-482811`

3. Clique em **"CREATE DATASET"** e crie os 3 datasets:

   **Dataset 1: northwind_bronze**
   - Dataset ID: `northwind_bronze`
   - Data location: `US`
   - Description: `Camada Bronze - Dados brutos do PostgreSQL`
   - Deixe outras op√ß√µes padr√£o
   - Clique em **CREATE DATASET**

   **Dataset 2: northwind_silver**
   - Dataset ID: `northwind_silver`
   - Data location: `US`
   - Description: `Camada Silver - Dados limpos e enriquecidos`
   - Clique em **CREATE DATASET**

   **Dataset 3: northwind_gold**
   - Dataset ID: `northwind_gold`
   - Data location: `US`
   - Description: `Camada Gold - Agrega√ß√µes de neg√≥cio`
   - Clique em **CREATE DATASET**

### **Op√ß√£o 2: Via bq CLI** (Se tiver instalado)

```bash
# Instalar Google Cloud SDK
snap install google-cloud-sdk

# Autenticar
gcloud auth login
gcloud config set project portifolio-482811

# Criar datasets
bq mk --dataset --location=US --description="Camada Bronze" northwind_bronze
bq mk --dataset --location=US --description="Camada Silver" northwind_silver
bq mk --dataset --location=US --description="Camada Gold" northwind_gold
```

### **Op√ß√£o 3: Dar Permiss√£o √† Service Account**

1. Acesse https://console.cloud.google.com/iam-admin/iam?project=portifolio-482811

2. Encontre sua Service Account (algo como `***@portifolio-482811.iam.gserviceaccount.com`)

3. Clique em **EDIT** (√≠cone de l√°pis)

4. Adicione o papel: **BigQuery Admin**

5. Salve e execute novamente:
   ```bash
   docker exec airflow-webserver python /tmp/create_bigquery_datasets.py
   ```

---

## ‚úÖ Depois de Criar os Datasets

### 1. Testar o Script de Ingest√£o

```bash
docker exec airflow-webserver bash -c "cd /opt/airflow/scripts && python postgres_to_bigquery.py"
```

**Sa√≠da esperada:**
```
‚úÖ Loader inicializado - Projeto: portifolio-482811
‚úÖ Sincroniza√ß√£o conclu√≠da: customers - 5 registros
‚úÖ Sincroniza√ß√£o conclu√≠da: orders - 5 registros
‚úÖ Sincroniza√ß√£o conclu√≠da: products - 10 registros
...
==================================================
Total de tabelas: 8
Sucesso: 8
Falhas: 0
Total de registros: 51
==================================================
```

### 2. Verificar Dados no BigQuery

```bash
# Via console: https://console.cloud.google.com/bigquery

# Ou via Python no container:
docker exec airflow-webserver python -c "
from google.cloud import bigquery
from google.oauth2 import service_account

credentials = service_account.Credentials.from_service_account_file(
    '/opt/airflow/gcp-key.json'
)
client = bigquery.Client(credentials=credentials, project='portifolio-482811')

query = '''
SELECT table_name, row_count
FROM \`portifolio-482811.northwind_bronze.__TABLES__\`
ORDER BY table_name
'''

for row in client.query(query):
    print(f'{row.table_name}: {row.row_count} registros')
"
```

### 3. Executar DAG no Airflow

1. Acesse http://localhost:8080
   - Username: `airflow`
   - Password: `airflow`

2. Encontre o DAG `northwind_data_pipeline`

3. Ative o DAG (toggle no lado esquerdo)

4. Clique em **"Trigger DAG"** (bot√£o ‚ñ∂Ô∏è)

5. Acompanhe a execu√ß√£o:
   - ‚úÖ `sync_postgres_to_bigquery` - Ingest√£o dos dados
   - ‚úÖ `dbt_deps` - Instalar depend√™ncias do dbt
   - ‚úÖ `dbt_run_bronze` - Criar views/tabelas Bronze
   - ‚úÖ `dbt_test_bronze` - Testes de qualidade
   - ‚úÖ `dbt_run_silver` - Transforma√ß√µes Silver
   - ‚úÖ `dbt_test_silver` - Testes Silver
   - ‚úÖ `dbt_run_gold` - Agrega√ß√µes Gold
   - ‚úÖ `dbt_test_gold` - Testes finais
   - ‚úÖ `dbt_docs_generate` - Gerar documenta√ß√£o

### 4. Executar dbt Manualmente (Alternativa)

```bash
# Entrar no container
docker exec -it airflow-webserver bash

# Navegar para o projeto dbt
cd /opt/airflow/dbt/northwind_dw

# Instalar depend√™ncias
dbt deps --profiles-dir /opt/airflow/dbt

# Executar todos os modelos
dbt run --profiles-dir /opt/airflow/dbt

# Executar apenas uma camada
dbt run --select tag:bronze --profiles-dir /opt/airflow/dbt
dbt run --select tag:silver --profiles-dir /opt/airflow/dbt
dbt run --select tag:gold --profiles-dir /opt/airflow/dbt

# Executar testes
dbt test --profiles-dir /opt/airflow/dbt

# Gerar documenta√ß√£o
dbt docs generate --profiles-dir /opt/airflow/dbt
```

---

## üìä Valida√ß√£o Final

### Checar Tabelas Criadas

```sql
-- No BigQuery Console
-- https://console.cloud.google.com/bigquery?project=portifolio-482811

-- Bronze Layer (dados brutos)
SELECT * FROM `portifolio-482811.northwind_bronze.bronze_customers` LIMIT 5;
SELECT * FROM `portifolio-482811.northwind_bronze.bronze_orders` LIMIT 5;

-- Silver Layer (dados limpos)
SELECT * FROM `portifolio-482811.northwind_silver.dim_customers` LIMIT 5;
SELECT * FROM `portifolio-482811.northwind_silver.dim_products` LIMIT 5;
SELECT * FROM `portifolio-482811.northwind_silver.fact_sales` LIMIT 10;

-- Gold Layer (m√©tricas de neg√≥cio)
SELECT * FROM `portifolio-482811.northwind_gold.revenue_by_customer` ORDER BY total_revenue DESC;
SELECT * FROM `portifolio-482811.northwind_gold.sales_summary`;
```

### Contagem de Registros Esperada

| Camada | Tabela | Registros Esperados |
|--------|--------|---------------------|
| Bronze | bronze_customers | 5 |
| Bronze | bronze_orders | 5 |
| Bronze | bronze_products | 10 |
| Bronze | bronze_employees | 5 |
| Silver | dim_customers | 5 |
| Silver | dim_products | 10 |
| Silver | fact_sales | 11 |
| Gold | revenue_by_customer | 3-5 |
| Gold | sales_summary | 1 |

---

## üéØ Pr√≥ximas A√ß√µes

Depois que o pipeline funcionar:

1. **Publicar no GitHub**
   ```bash
   git init
   git add .
   git commit -m "Pipeline de dados Northwind com Python e dbt"
   git remote add origin https://github.com/seu-usuario/northwind-pipeline
   git push -u origin main
   ```

2. **Criar README impressionante** (j√° temos!)

3. **Adicionar ao LinkedIn** com link do GitHub

4. **Preparar apresenta√ß√£o** para entrevistas

---

**Status**: ‚è≥ **Aguardando cria√ß√£o dos datasets no BigQuery**

Depois que criar os 3 datasets, execute:
```bash
docker exec airflow-webserver bash -c "cd /opt/airflow/scripts && python postgres_to_bigquery.py"
```
