# ğŸš€ Northwind Data Pipeline - Projeto de Engenharia de Dados

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python&logoColor=white)](https://www.python.org/)
[![dbt](https://img.shields.io/badge/dbt-1.7.4-orange?logo=dbt&logoColor=white)](https://www.getdbt.com/)
[![Airflow](https://img.shields.io/badge/Airflow-2.8.0-017CEE?logo=apache-airflow&logoColor=white)](https://airflow.apache.org/)
[![BigQuery](https://img.shields.io/badge/BigQuery-GCP-4285F4?logo=google-cloud&logoColor=white)](https://cloud.google.com/bigquery)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-336791?logo=postgresql&logoColor=white)](https://www.postgresql.org/)

Pipeline completo de engenharia de dados end-to-end implementando arquitetura Medallion (Bronze, Silver, Gold) com stack moderna de tecnologias em nuvem.

> **âœ¨ Projeto de PortfÃ³lio** | DemonstraÃ§Ã£o de habilidades em Data Engineering com foco em ETL/ELT, orquestraÃ§Ã£o e transformaÃ§Ã£o de dados.

## ğŸ¯ Resultados do Projeto

- âœ… **51 registros** processados atravÃ©s de 3 camadas de transformaÃ§Ã£o
- âœ… **8 tabelas Bronze** â†’ **4 tabelas Silver** â†’ **5 tabelas Gold**
- âœ… **Pipeline automatizado** executando transformaÃ§Ãµes dbt via Airflow
- âœ… **Arquitetura Medallion** implementada no Google BigQuery
- âœ… **Testes de qualidade** validando integridade dos dados

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Arquitetura](#arquitetura)
- [Tecnologias](#tecnologias)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [ConfiguraÃ§Ã£o](#configuraÃ§Ã£o)
- [ExecuÃ§Ã£o](#execuÃ§Ã£o)
- [Camadas de Dados](#camadas-de-dados)
- [Monitoramento](#monitoramento)
- [Testes](#testes)

## ğŸ¯ VisÃ£o Geral

Este projeto implementa um **pipeline de dados end-to-end** utilizando a clÃ¡ssica base de dados **Northwind** como fonte. O objetivo Ã© demonstrar as melhores prÃ¡ticas de engenharia de dados moderna, incluindo:

- âœ… **IngestÃ£o de dados** automatizada com Python (PostgreSQL â†’ BigQuery)
- âœ… **TransformaÃ§Ã£o de dados** com dbt usando arquitetura Medallion
- âœ… **OrquestraÃ§Ã£o** com Apache Airflow e TaskGroups
- âœ… **Armazenamento** escalÃ¡vel no Google BigQuery
- âœ… **ContainerizaÃ§Ã£o** com Docker Compose (4 containers)
- âœ… **Testes de qualidade** automatizados com dbt
- âœ… **Versionamento** com Git/GitHub

### ğŸ“Š MÃ©tricas do Pipeline

| MÃ©trica | Valor |
|---------|-------|
| **Registros Processados** | 51 registros |
| **Tabelas Bronze** | 8 tabelas (dados brutos) |
| **Modelos Silver** | 4 dimensÃµes/fatos |
| **AgregaÃ§Ãµes Gold** | 5 mÃ©tricas de negÃ³cio |
| **Tempo de ExecuÃ§Ã£o** | ~2-3 minutos |
| **Testes de Qualidade** | 16 testes implementados |

### Fluxo de Dados

```
PostgreSQL (Source - 51 records)
    â†“
Python Script (ETL Customizado)
    â†“
BigQuery Bronze (Raw Data - 8 tables)
    â†“
dbt Silver (Cleaned & Modeled - 4 models)
    â†“
dbt Gold (Business Aggregations - 5 models)
    â†“
Analytics & BI Tools
```

## ğŸ—ï¸ Arquitetura

### Arquitetura Medallion

O projeto implementa a arquitetura Medallion em trÃªs camadas:

#### ğŸ¥‰ Bronze Layer (Dados Brutos)
- Dados brutos ingeridos do PostgreSQL via script Python customizado
- MÃ­nima ou nenhuma transformaÃ§Ã£o aplicada
- Preserva histÃ³rico completo com metadata de ingestÃ£o
- **8 tabelas**: `bronze_customers`, `bronze_orders`, `bronze_products`, `bronze_categories`, `bronze_employees`, `bronze_suppliers`, `bronze_shippers`, `bronze_order_details`

#### ğŸ¥ˆ Silver Layer (Dados Limpos)
- Dados limpos, padronizados e modelados
- AplicaÃ§Ã£o de surrogate keys e normalizaÃ§Ã£o
- ImplementaÃ§Ã£o de modelos dimensionais (star schema)
- **4 modelos**: `silver_dim_customers`, `silver_dim_products`, `silver_dim_employees`, `silver_fact_orders`

#### ğŸ¥‡ Gold Layer (AgregaÃ§Ãµes de NegÃ³cio)
- MÃ©tricas e KPIs prontos para consumo
- Dados otimizados para dashboards e anÃ¡lises
- AgregaÃ§Ãµes prÃ©-calculadas para performance
- **5 agregaÃ§Ãµes**: `gold_customer_revenue`, `gold_employee_performance`, `gold_product_performance`, `gold_revenue_by_category`, `gold_revenue_by_supplier`

### Diagrama de Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚     â”‚                  â”‚     â”‚                  â”‚
â”‚  PostgreSQL â”‚â”€â”€â”€â”€â–¶â”‚  Python ETL      â”‚â”€â”€â”€â”€â–¶â”‚  BigQuery Bronze â”‚
â”‚  (Source)   â”‚     â”‚  (431 lines)     â”‚     â”‚   (8 tables)     â”‚
â”‚  51 records â”‚     â”‚  Custom Script   â”‚     â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â”‚
                                                       â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         dbt Transformations              â”‚
                    â”‚                                          â”‚
                    â”‚  Silver Layer (4 models)                â”‚
                    â”‚  â””â”€ Dimensions & Facts                  â”‚
                    â”‚                                          â”‚
                    â”‚  Gold Layer (5 models)                  â”‚
                    â”‚  â””â”€ Business Aggregations               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â”‚ Orchestrated by
                                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    Apache Airflow 2.8.0             â”‚
              â”‚                                     â”‚
              â”‚  âœ“ DAG with TaskGroups              â”‚
              â”‚  âœ“ BashOperator for dbt             â”‚
              â”‚  âœ“ Automated Testing                â”‚
              â”‚  âœ“ Pipeline Monitoring              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tecnologias

| Tecnologia | VersÃ£o | FunÃ§Ã£o | Detalhes |
|------------|--------|--------|----------|
| **PostgreSQL** | 15 | Banco de dados fonte | Base Northwind com 51 registros |
| **Python** | 3.11 | ETL Customizado | Script de 431 linhas para ingestÃ£o |
| **Google BigQuery** | - | Data Warehouse | 3 datasets (Bronze/Silver/Gold) |
| **dbt** | 1.7.4 | TransformaÃ§Ãµes | 17 modelos + testes de qualidade |
| **Apache Airflow** | 2.8.0 | OrquestraÃ§Ã£o | LocalExecutor + TaskGroups |
| **Docker** | Latest | ContainerizaÃ§Ã£o | 4 containers coordenados |
| **Docker Compose** | Latest | OrchestraÃ§Ã£o de containers | Gerenciamento de serviÃ§os |

### ğŸ”§ Stack TÃ©cnica Detalhada

- **ETL**: `psycopg2`, `google-cloud-bigquery`, pandas para transformaÃ§Ãµes
- **dbt**: dbt-core + dbt-bigquery adapter + dbt-utils package
- **Airflow**: BashOperator, TaskGroups, dynamic task generation
- **Infraestrutura**: Docker Compose com volumes persistentes

## ğŸ“ Estrutura do Projeto

```
northwind-data-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ postgres/
â”‚   â””â”€â”€ init/
â”‚       â”œâ”€â”€ 01_schema.sql         # Schema do banco Northwind
â”‚       â””â”€â”€ 02_data.sql           # Dados de exemplo
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ northwind_pipeline_dag.py      # â­ DAG principal (295 linhas)
â”‚   â”‚   â”œâ”€â”€ northwind_monitoring_dag.py    # Monitoramento
â”‚   â”‚   â””â”€â”€ northwind_maintenance_dag.py   # ManutenÃ§Ã£o
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ postgres_to_bigquery.py        # â­ ETL Script (431 linhas)
â”‚   â”‚   â””â”€â”€ create_gcp_connection.py       # Setup GCP
â”‚   â”œâ”€â”€ entrypoint.sh                      # Auto-instalaÃ§Ã£o de dependÃªncias
â”‚   â””â”€â”€ requirements.txt                   # DependÃªncias Python
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ profiles.yml              # ConfiguraÃ§Ã£o de conexÃ£o dbt
â”‚   â””â”€â”€ northwind_dw/
â”‚       â”œâ”€â”€ dbt_project.yml       # ConfiguraÃ§Ã£o do projeto dbt
â”‚       â”œâ”€â”€ packages.yml          # Pacotes dbt
â”‚       â””â”€â”€ models/
â”‚           â”œâ”€â”€ bronze/           # Camada Bronze (Raw)
â”‚           â”‚   â”œâ”€â”€ bronze_customers.sql
â”‚           â”‚   â”œâ”€â”€ bronze_orders.sql
â”‚           â”‚   â”œâ”€â”€ bronze_products.sql
â”‚           â”‚   â””â”€â”€ ...
â”‚           â”œâ”€â”€ silver/           # Camada Silver (Cleaned)
â”‚           â”‚   â”œâ”€â”€ silver_dim_customers.sql
â”‚           â”‚   â”œâ”€â”€ silver_dim_products.sql
â”‚           â”‚   â”œâ”€â”€ silver_fact_orders.sql
â”‚           â”‚   â””â”€â”€ ...
â”‚           â””â”€â”€ gold/             # Camada Gold (Business)
â”‚               â”œâ”€â”€ gold_sales_by_country.sql
â”‚               â”œâ”€â”€ gold_customer_analytics.sql
â”‚               â”œâ”€â”€ gold_employee_performance.sql
â”‚               â””â”€â”€ ...
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ SETUP.md                  # Guia de instalaÃ§Ã£o detalhado
    â”œâ”€â”€ ARCHITECTURE.md           # DocumentaÃ§Ã£o da arquitetura
    â””â”€â”€ DATA_DICTIONARY.md        # DicionÃ¡rio de dados
```

## ğŸ“¦ PrÃ©-requisitos

- **Docker** e **Docker Compose** instalados
- **Google Cloud Platform** account com BigQuery habilitado
- **Service Account Key** do GCP com permissÃµes:
  - BigQuery Data Editor
  - BigQuery Job User
- **Git** para versionamento
- MÃ­nimo de **8GB RAM** e **20GB** de espaÃ§o em disco

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/Gads1208/northwind-data-pipeline.git
cd northwind-data-pipeline
```

### 2. Configure as variÃ¡veis de ambiente

```bash
cp .env.example .env
```

Edite o arquivo `.env` com suas credenciais:

```bash
# Google Cloud Configuration
GCP_PROJECT_ID=seu-project-id
GCP_DATASET_BRONZE=northwind_bronze
GCP_DATASET_SILVER=northwind_silver
GCP_DATASET_GOLD=northwind_gold
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json

# PostgreSQL Configuration
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=northwind
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
```

### 3. Configure o Google Cloud

Crie os datasets no BigQuery (regiÃ£o US):

```bash
# Criar datasets
bq mk --location=US --dataset ${GCP_PROJECT_ID}:northwind_bronze
bq mk --location=US --dataset ${GCP_PROJECT_ID}:northwind_silver
bq mk --location=US --dataset ${GCP_PROJECT_ID}:northwind_gold
```

**Importante**: Configure as permissÃµes da Service Account:
- BigQuery Data Editor
- BigQuery Job User

### 4. Coloque sua Service Account Key

Copie sua service account key JSON para a raiz do projeto:

```bash
cp /caminho/para/sua/service-account-key.json ./gcp-key.json
```

### 5. Inicie os serviÃ§os com Docker

```bash
# Suba todos os containers
docker-compose up -d

# Verifique se todos estÃ£o rodando
docker-compose ps
```

Aguarde ~2 minutos para os serviÃ§os iniciarem completamente.

### 6. Acesse as interfaces

- **Airflow**: http://localhost:8080 (user: `airflow` / pass: `airflow`)
- **PostgreSQL**: localhost:5432 (user: `postgres` / pass: `postgres`)

## âš™ï¸ ConfiguraÃ§Ã£o

### Executar o Pipeline

#### MÃ©todo 1: Via Airflow UI (Recomendado)

1. Acesse http://localhost:8080
2. Localize o DAG `northwind_pipeline`
3. Clique em "Trigger DAG" (Ã­cone de play)
4. Acompanhe a execuÃ§Ã£o em tempo real

#### MÃ©todo 2: Via Linha de Comando

```bash
# Entre no container do Airflow
docker exec -it northwind-data-pipeline-airflow-scheduler-1 bash

# Trigger manual da DAG
airflow dags trigger northwind_pipeline
```

### Fluxo de ExecuÃ§Ã£o da DAG

O pipeline executa as seguintes tarefas em sequÃªncia:

1. **ingest_bronze** â†’ IngestÃ£o Python (PostgreSQL â†’ BigQuery Bronze)
2. **create_profile** â†’ CriaÃ§Ã£o dinÃ¢mica do profiles.yml
3. **install_deps** â†’ InstalaÃ§Ã£o dos pacotes dbt (dbt_utils)
4. **dbt_debug** â†’ ValidaÃ§Ã£o da conexÃ£o dbt
5. **silver_layer** â†’ TaskGroup com 4 modelos Silver
6. **gold_layer** â†’ TaskGroup com 5 modelos Gold
7. **summary** â†’ Log de finalizaÃ§Ã£o
8. **run_tests** â†’ Testes de qualidade dbt

## ğŸ® ExecuÃ§Ã£o

### Verificar Status dos ServiÃ§os

```bash
# Ver status dos containers
docker-compose ps

# Ver logs em tempo real
docker logs -f northwind-data-pipeline-airflow-webserver-1

# Verificar saÃºde do PostgreSQL
docker exec -it northwind-data-pipeline-postgres-1 psql -U postgres -d northwind -c "SELECT COUNT(*) FROM customers;"
```

### ExecuÃ§Ã£o Manual das TransformaÃ§Ãµes dbt

Se vocÃª quiser executar apenas o dbt sem o Airflow:

```bash
# Entre no container
docker exec -it northwind-data-pipeline-airflow-scheduler-1 bash

# Execute as transformaÃ§Ãµes
cd /opt/airflow/dbt/northwind_dw

# Instalar dependÃªncias
dbt deps --profiles-dir /opt/airflow/dbt

# Executar todos os modelos
dbt run --profiles-dir /opt/airflow/dbt

# Executar apenas Silver
dbt run --select silver_* --profiles-dir /opt/airflow/dbt

# Executar apenas Gold
dbt run --select gold_* --profiles-dir /opt/airflow/dbt

# Executar os testes
dbt test --profiles-dir /opt/airflow/dbt

# Gerar documentaÃ§Ã£o
dbt docs generate --profiles-dir /opt/airflow/dbt
```

### ExecuÃ§Ã£o AutomÃ¡tica

O pipeline pode ser configurado para executar automaticamente (ajuste no arquivo DAG):

```python
schedule_interval='0 2 * * *',  # Diariamente Ã s 2h AM
```

## ğŸ“Š Camadas de Dados

### Bronze Layer (Raw Data)

Dados brutos ingeridos do PostgreSQL via script Python customizado:

| Tabela | DescriÃ§Ã£o | Registros |
|--------|-----------|-----------|
| `bronze_categories` | Categorias de produtos | 8 |
| `bronze_customers` | Clientes | 91 |
| `bronze_employees` | FuncionÃ¡rios | 9 |
| `bronze_orders` | Pedidos | 830 |
| `bronze_order_details` | Detalhes dos pedidos | 2155 |
| `bronze_products` | Produtos | 77 |
| `bronze_suppliers` | Fornecedores | 29 |
| `bronze_shippers` | Transportadoras | 3 |

### Silver Layer (Modeled Data)

Dados modelados em dimensÃµes e fatos:

| Modelo | Tipo | DescriÃ§Ã£o |
|--------|------|-----------|
| `silver_dim_customers` | DimensÃ£o | DimensÃ£o de clientes com surrogate key |
| `silver_dim_products` | DimensÃ£o | DimensÃ£o de produtos enriquecida |
| `silver_dim_employees` | DimensÃ£o | DimensÃ£o de funcionÃ¡rios |
| `silver_fact_orders` | Fato | Fato de pedidos com mÃ©tricas |

### Gold Layer (Business Metrics)

AgregaÃ§Ãµes de negÃ³cio prontas para anÃ¡lise:

| Modelo | DescriÃ§Ã£o | MÃ©tricas |
|--------|-----------|----------|
| `gold_customer_revenue` | Receita por cliente | Total revenue, order count, avg order value |
| `gold_employee_performance` | Performance de vendedores | Orders handled, total revenue, avg order |
| `gold_product_performance` | Performance de produtos | Units sold, total revenue, avg price |
| `gold_revenue_by_category` | Receita por categoria | Revenue per category, product count |
| `gold_revenue_by_supplier` | Receita por fornecedor | Revenue per supplier, order count |

## ğŸ“ˆ Monitoramento

### Airflow UI - Acompanhamento em Tempo Real

Acesse http://localhost:8080 para visualizar:
- âœ… Status de execuÃ§Ã£o das DAGs
- âœ… Logs detalhados de cada task
- âœ… GrÃ¡fico de dependÃªncias (Graph View)
- âœ… HistÃ³rico de execuÃ§Ãµes (Gantt Chart)

### Logs dos Containers

```bash
# Airflow Scheduler
docker logs -f northwind-data-pipeline-airflow-scheduler-1

# Airflow Webserver
docker logs -f northwind-data-pipeline-airflow-webserver-1

# PostgreSQL
docker logs -f northwind-data-pipeline-postgres-1
```

### Verificar Dados no BigQuery

```sql
-- Verificar contagem de registros por camada
SELECT 
  'Bronze - Orders' as table_name,
  COUNT(*) as record_count 
FROM `portifolio-482811.northwind_bronze.bronze_orders`

UNION ALL

SELECT 
  'Silver - Fact Orders' as table_name,
  COUNT(*) as record_count 
FROM `portifolio-482811.northwind_silver.silver_fact_orders`

UNION ALL

SELECT 
  'Gold - Customer Revenue' as table_name,
  COUNT(*) as record_count 
FROM `portifolio-482811.northwind_gold.gold_customer_revenue`;
```

### Exemplo de Consulta AnalÃ­tica

```sql
-- Top 10 clientes por receita
SELECT 
  customer_id,
  total_revenue,
  order_count,
  avg_order_value
FROM `portifolio-482811.northwind_gold.gold_customer_revenue`
ORDER BY total_revenue DESC
LIMIT 10;
```

## ğŸ§ª Testes

### Testes Implementados

O projeto inclui **16 testes de qualidade de dados**:

#### Testes de Integridade (Bronze Layer)
- âœ… Uniqueness de primary keys
- âœ… Not null em campos obrigatÃ³rios

#### Testes de NegÃ³cio (Silver Layer)
- âœ… ValidaÃ§Ã£o de surrogate keys
- âœ… ConsistÃªncia de foreign keys
- âœ… ValidaÃ§Ã£o de customer_id, product_id, employee_id

### Executar Testes

```bash
# Todos os testes
docker exec -it northwind-data-pipeline-airflow-scheduler-1 bash -c \
  "cd /opt/airflow/dbt/northwind_dw && dbt test --profiles-dir /opt/airflow/dbt"

# Testes de uma camada especÃ­fica
dbt test --select silver_* --profiles-dir /opt/airflow/dbt

# Teste de um modelo especÃ­fico
dbt test --select silver_dim_customers --profiles-dir /opt/airflow/dbt
```

### Exemplo de SaÃ­da

```
Completed successfully

Done. PASS=16 WARN=0 ERROR=0 SKIP=0 TOTAL=16
```

## ğŸ”§ ManutenÃ§Ã£o e Troubleshooting

### Comandos Ãšteis

```bash
# Reiniciar apenas o Airflow
docker-compose restart airflow-webserver airflow-scheduler

# Ver uso de recursos
docker stats

# Limpar logs antigos
docker exec -it northwind-data-pipeline-airflow-scheduler-1 \
  find /opt/airflow/logs -type f -mtime +7 -delete
```

### Problemas Comuns

#### âŒ Erro: "Permission denied" no BigQuery
**SoluÃ§Ã£o**: Verifique as permissÃµes da Service Account no GCP

#### âŒ Erro: "dbt command not found"
**SoluÃ§Ã£o**: O entrypoint.sh instala automaticamente. Reinicie o container:
```bash
docker-compose restart airflow-scheduler
```

#### âŒ DAG nÃ£o aparece no Airflow
**SoluÃ§Ã£o**: Verifique se o arquivo DAG tem erros de sintaxe:
```bash
docker exec -it northwind-data-pipeline-airflow-scheduler-1 \
  python /opt/airflow/dags/northwind_pipeline_dag.py
```

### Limpar Ambiente Completamente

```bash
# Parar todos os containers
docker-compose down

# Remover volumes (ATENÃ‡ÃƒO: apaga todos os dados!)
docker-compose down -v

# Remover imagens
docker-compose down --rmi all

# Reiniciar do zero
docker-compose up -d
```

## ğŸ“š Recursos e ReferÃªncias

### DocumentaÃ§Ã£o Oficial
- [dbt Documentation](https://docs.getdbt.com/) - TransformaÃ§Ãµes e testes
- [Apache Airflow Docs](https://airflow.apache.org/docs/) - OrquestraÃ§Ã£o
- [BigQuery Documentation](https://cloud.google.com/bigquery/docs) - Data Warehouse
- [Docker Compose](https://docs.docker.com/compose/) - ContainerizaÃ§Ã£o

### Conceitos Aplicados
- **Medallion Architecture**: Bronze â†’ Silver â†’ Gold layers
- **Star Schema**: Modelagem dimensional
- **ELT Pattern**: Extract-Load-Transform
- **Data Quality**: Testes automatizados com dbt
- **Infrastructure as Code**: Docker Compose

### Artigos Relacionados
- [Medallion Architecture Best Practices](https://www.databricks.com/glossary/medallion-architecture)
- [dbt Best Practices](https://docs.getdbt.com/guides/best-practices)
- [Airflow Task Groups](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#taskgroups)

## ğŸ“ Aprendizados e Desafios

### Desafios TÃ©cnicos Superados

1. **IntegraÃ§Ã£o dbt + Airflow**: ImplementaÃ§Ã£o usando BashOperator ao invÃ©s de Astronomer Cosmos para maior controle
2. **SerializaÃ§Ã£o JSON**: Tratamento de tipos Decimal e datetime para BigQuery
3. **Docker Dependencies**: ConfiguraÃ§Ã£o de entrypoint.sh para instalaÃ§Ã£o automÃ¡tica de git e dependÃªncias
4. **dbt Packages**: CorreÃ§Ã£o do formato packages.yml para instalaÃ§Ã£o do dbt_utils

### Skills Demonstradas

- âœ… Python ETL development (431 linhas)
- âœ… SQL transformations com dbt (17 modelos)
- âœ… Airflow DAG development com TaskGroups
- âœ… Docker Compose orchestration
- âœ… Google Cloud BigQuery
- âœ… Git version control
- âœ… Data quality testing
- âœ… Documentation

## ğŸš€ PrÃ³ximos Passos

Melhorias futuras planejadas:

- [ ] Implementar CI/CD com GitHub Actions
- [ ] Adicionar Great Expectations para validaÃ§Ãµes avanÃ§adas
- [ ] Criar dashboard no Looker Studio/Power BI
- [ ] Implementar incremental models no dbt
- [ ] Adicionar alertas via Slack/Email
- [ ] Implementar data lineage tracking
- [ ] Adicionar mais testes de qualidade
- [ ] Otimizar particionamento no BigQuery

## ğŸ‘¤ Autor

**Guilherme Alves da Silva**
- ğŸ“§ Email: gads1208@gmail.com
- ğŸ™ GitHub: [@Gads1208](https://github.com/Gads1208)
- ğŸ”— LinkedIn: [Seu Perfil](https://linkedin.com/in/seu-perfil)

> ğŸ’¼ Este projeto foi desenvolvido como parte do meu portfÃ³lio de Data Engineering, demonstrando habilidades em ETL/ELT, orquestraÃ§Ã£o de dados, transformaÃ§Ãµes SQL e infraestrutura em nuvem.

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ™ Agradecimentos

- Base de dados **Northwind** da Microsoft - Dataset clÃ¡ssico para demonstraÃ§Ãµes
- Comunidade **dbt** - Framework incrÃ­vel para transformaÃ§Ãµes
- **Apache Airflow** - OrquestraÃ§Ã£o de pipelines de dados
- **Google Cloud Platform** - Infraestrutura BigQuery

---

<div align="center">

### â­ Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela!

**Made with â¤ï¸ and â˜• by Guilherme**

[â¬† Voltar ao topo](#-northwind-data-pipeline---projeto-de-engenharia-de-dados)

</div>
