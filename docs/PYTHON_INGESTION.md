# ðŸ Script Python para IngestÃ£o de Dados

Este documento explica o script Python customizado que substitui o Airbyte para ingestÃ£o de dados.

## ðŸ“ LocalizaÃ§Ã£o

```
airflow/scripts/postgres_to_bigquery.py
```

## ðŸŽ¯ Objetivo

Extrair dados do PostgreSQL (Northwind) e carregar no Google BigQuery (camada Bronze) de forma automatizada, com metadados de rastreamento e tratamento de erros.

## ðŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚
â”‚   (Northwind)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“ psycopg2
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgresToBigQueryLoader   â”‚
â”‚                             â”‚
â”‚  1. get_table_schema()      â”‚
â”‚  2. extract_table_data()    â”‚
â”‚  3. create_or_update_table()â”‚
â”‚  4. load_data_to_bigquery() â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“ google-cloud-bigquery
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BigQuery      â”‚
â”‚ (Bronze Layer)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ”§ Funcionalidades

### 1. **Classe PostgresToBigQueryLoader**

Gerencia todo o processo de sincronizaÃ§Ã£o.

```python
loader = PostgresToBigQueryLoader(
    bigquery_project_id='meu-projeto',
    bigquery_dataset='northwind_bronze',
    service_account_path='/path/to/gcp-key.json'
)
```

### 2. **ExtraÃ§Ã£o de Dados**

```python
data = loader.extract_table_data('customers')
# Retorna: [{'customer_id': 'ALFKI', 'company_name': 'Alfreds', ...}, ...]
```

**Features**:
- âœ… Usa `RealDictCursor` para retornar dicionÃ¡rios
- âœ… Adiciona metadados `_airbyte_extracted_at` e `_airbyte_loaded_at`
- âœ… Gerenciamento automÃ¡tico de conexÃµes
- âœ… Tratamento de erros robusto

### 3. **Mapeamento de Schema**

Converte automaticamente tipos PostgreSQL â†’ BigQuery:

| PostgreSQL | BigQuery | Exemplo |
|-----------|----------|---------|
| VARCHAR | STRING | 'ALFKI' |
| INTEGER | INTEGER | 12345 |
| NUMERIC | NUMERIC | 123.45 |
| DATE | DATE | 2024-01-15 |
| BOOLEAN | BOOLEAN | true |
| TIMESTAMP | TIMESTAMP | 2024-01-15 10:30:00 |

### 4. **Carga no BigQuery**

```python
loader.load_data_to_bigquery('customers', data)
```

**ConfiguraÃ§Ãµes**:
- `WRITE_TRUNCATE`: Sobrescreve dados existentes (full refresh)
- `NEWLINE_DELIMITED_JSON`: Formato eficiente
- CriaÃ§Ã£o automÃ¡tica de tabelas se nÃ£o existirem

### 5. **SincronizaÃ§Ã£o Completa**

```python
results = loader.sync_all_tables()
# Sincroniza: customers, orders, products, employees, suppliers, categories, shippers, order_details
```

## ðŸ“Š Tabelas Suportadas

| Tabela | Registros Esperados | Chave PrimÃ¡ria |
|--------|---------------------|----------------|
| customers | ~90 | customer_id |
| orders | ~800 | order_id |
| order_details | ~2000 | order_id + product_id |
| products | ~77 | product_id |
| employees | ~10 | employee_id |
| suppliers | ~29 | supplier_id |
| categories | ~8 | category_id |
| shippers | ~3 | shipper_id |

## ðŸ”„ IntegraÃ§Ã£o com Airflow

O script Ã© chamado automaticamente pelo DAG `northwind_data_pipeline`:

```python
# airflow/dags/northwind_pipeline_dag.py

def sync_postgres_to_bigquery():
    from postgres_to_bigquery import PostgresToBigQueryLoader
    
    loader = PostgresToBigQueryLoader(
        bigquery_project_id=os.getenv('GCP_PROJECT_ID'),
        bigquery_dataset='northwind_bronze'
    )
    
    results = loader.sync_all_tables()
    
    if results['failed'] > 0:
        raise Exception(f"Falha em {results['failed']} tabelas")
    
    return results

sync_data = PythonOperator(
    task_id='sync_postgres_to_bigquery',
    python_callable=sync_postgres_to_bigquery,
)
```

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. VariÃ¡veis de Ambiente

Configure no `docker-compose.yml` ou `.env`:

```bash
GCP_PROJECT_ID=meu-projeto-gcp
BIGQUERY_DATASET=northwind_bronze
GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/gcp-key.json

POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=northwind
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
```

### 2. Credenciais GCP

1. Crie uma Service Account no GCP
2. DÃª permissÃµes de BigQuery Admin
3. Baixe a chave JSON
4. Salve como `gcp-key.json` na raiz do projeto

### 3. DependÃªncias

Instaladas automaticamente via `requirements.txt`:

```txt
psycopg2-binary==2.9.9
google-cloud-bigquery==3.14.1
google-auth==2.25.2
```

## ðŸ§ª Testes

### Teste Local (Fora do Docker)

```bash
cd /home/gas/Guilherme/potifolio/northwind-data-pipeline/airflow/scripts

# Configurar variÃ¡veis
export GCP_PROJECT_ID=meu-projeto
export POSTGRES_HOST=localhost
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/gcp-key.json

# Executar
python postgres_to_bigquery.py
```

### Teste no Docker

```bash
docker exec -it airflow-webserver bash
cd /opt/airflow/scripts
python postgres_to_bigquery.py
```

### SaÃ­da Esperada

```
2024-01-15 10:30:00 - INFO - Loader inicializado - Projeto: meu-projeto, Dataset: northwind_bronze
2024-01-15 10:30:01 - INFO - ConexÃ£o com PostgreSQL estabelecida
2024-01-15 10:30:01 - INFO - ExtraÃ­dos 91 registros da tabela customers
2024-01-15 10:30:02 - INFO - Tabela meu-projeto.northwind_bronze.bronze_customers criada
2024-01-15 10:30:03 - INFO - Carregados 91 registros na tabela bronze_customers
2024-01-15 10:30:03 - INFO - SincronizaÃ§Ã£o concluÃ­da: customers - 91 registros em 2.15s
...
==================================================
RESUMO DA SINCRONIZAÃ‡ÃƒO
==================================================
Total de tabelas: 8
Sucesso: 8
Falhas: 0
Total de registros: 3215
==================================================
```

## ðŸ“ˆ Monitoramento

### Logs no Airflow

Visualize logs detalhados na UI do Airflow:

```
http://localhost:8080 â†’ DAGs â†’ northwind_data_pipeline â†’ Task: sync_postgres_to_bigquery â†’ Logs
```

### VerificaÃ§Ã£o no BigQuery

```sql
-- Contar registros por tabela
SELECT 
  table_name,
  row_count,
  size_bytes / 1024 / 1024 as size_mb
FROM `seu-projeto.northwind_bronze.__TABLES__`;

-- Verificar metadados
SELECT 
  COUNT(*) as total,
  MIN(_airbyte_extracted_at) as first_extraction,
  MAX(_airbyte_extracted_at) as last_extraction
FROM `seu-projeto.northwind_bronze.bronze_customers`;
```

## ðŸš€ Vantagens desta Abordagem

### vs. Airbyte

| Aspecto | Airbyte | Script Python |
|---------|---------|---------------|
| **Setup** | Complexo (10+ containers) | Simples (incluÃ­do no Airflow) |
| **ManutenÃ§Ã£o** | Alta | Baixa |
| **CustomizaÃ§Ã£o** | Limitada | Total |
| **Overhead** | Alto (~2GB RAM) | Baixo (~100MB RAM) |
| **Debugging** | DifÃ­cil | FÃ¡cil (logs diretos) |
| **DependÃªncias** | Muitas | MÃ­nimas |

### BenefÃ­cios para PortfÃ³lio

âœ… **Demonstra habilidades em Python**
âœ… **Mostra conhecimento de APIs (psycopg2, BigQuery)**
âœ… **CÃ³digo limpo e bem documentado**
âœ… **SoluÃ§Ã£o sob medida para o problema**
âœ… **FÃ¡cil de explicar em entrevistas**

## ðŸ”§ PersonalizaÃ§Ã£o

### Adicionar Nova Tabela

1. Adicione o schema em `get_table_schema()`:

```python
schemas = {
    'nova_tabela': [
        bigquery.SchemaField('id', 'INTEGER', mode='REQUIRED'),
        bigquery.SchemaField('nome', 'STRING'),
        bigquery.SchemaField('_airbyte_extracted_at', 'TIMESTAMP'),
        bigquery.SchemaField('_airbyte_loaded_at', 'TIMESTAMP'),
    ]
}
```

2. Adicione Ã  lista em `sync_all_tables()`:

```python
tables = [
    'customers',
    'orders',
    # ...
    'nova_tabela'  # Nova!
]
```

### Alterar EstratÃ©gia de Carga

Por padrÃ£o usa `WRITE_TRUNCATE` (full refresh). Para incremental:

```python
job_config = bigquery.LoadJobConfig(
    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,  # Append
    # Ou
    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Replace
)
```

### Adicionar TransformaÃ§Ãµes

```python
def extract_table_data(self, table_name: str):
    # ... cÃ³digo existente ...
    
    # Aplicar transformaÃ§Ãµes
    for row in rows:
        row['_airbyte_extracted_at'] = current_time
        row['_airbyte_loaded_at'] = current_time
        
        # Exemplo: normalizar strings
        if 'company_name' in row:
            row['company_name'] = row['company_name'].strip().upper()
    
    return rows
```

## ðŸ› Troubleshooting

### Erro: "VariÃ¡vel GCP_PROJECT_ID nÃ£o configurada"

```bash
# Adicione ao .env
echo "GCP_PROJECT_ID=seu-projeto" >> .env

# Reinicie containers
docker-compose down
docker-compose up -d
```

### Erro: "Permission denied on BigQuery"

Verifique permissÃµes da Service Account:
- BigQuery Admin
- BigQuery Data Editor
- BigQuery Job User

### Erro: "Connection refused to PostgreSQL"

```bash
# Verifique se o container estÃ¡ rodando
docker ps | grep postgres

# Teste conexÃ£o
docker exec -it northwind-postgres psql -U postgres -d northwind -c "SELECT 1"
```

### Tabelas Vazias no BigQuery

```python
# Debug: Adicione prints
def extract_table_data(self, table_name: str):
    # ...
    print(f"DEBUG: ExtraÃ­dos {len(rows)} registros")
    return rows
```

## ðŸ“š ReferÃªncias

- [psycopg2 Documentation](https://www.psycopg.org/docs/)
- [Google Cloud BigQuery Python Client](https://googleapis.dev/python/bigquery/latest/)
- [Airflow PythonOperator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html)

---

**PrÃ³ximos passos**: Configure o GCP e execute o pipeline! ðŸš€
